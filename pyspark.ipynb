{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDuy4yGp2mMdPgiwhMPpC0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark on Colab\n",
        "\n",
        "‚ö°Ô∏è Apache Spark is a powerful distributed computing framework designed for processing large-scale data efficiently. It offers a wide range of functionalities for data manipulation and machine learning. As a Data Scientist, learning Spark can help you handle massive datasets and perform complex analyses with ease.\n",
        "\n",
        "‚òÅÔ∏è On the other hand, Google Colab provides a convenient and free platform for running Python code, particularly suited for those not interested in local installation or powerful hardware (i.e. beginners). It's a hosted Jupyter Notebook service, inheriting all the pros of Jupyter, such as interactivity, visualizations, and documentation capabilities.\n",
        "\n",
        "ü§ù Combining Spark with Google Colab can enhance your data science projects by leveraging Spark's scalability and Colab's user-friendly interface.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YRBjv4B3xBtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Spark\n",
        "\n"
      ],
      "metadata": {
        "id": "7UeLzwmZRwUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a number of ways to download Spark, the easiest way is to install `pyspark` and then use the `findspark` library which is necessary for loading Spark. Note, `pyspark` is the Python API for Apache Spark."
      ],
      "metadata": {
        "id": "6GaBqsNiwbOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install findspark --quiet\n",
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spM7S85t6Fc",
        "outputId": "bf6a65ed-4c0e-4b69-a114-d308551472f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "        .appName('ColabTest') \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "QgkyWUpvt9_Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark provides a suite of web user interfaces (UIs) that you can use to monitor the status and resource consumption of your Spark cluster. Because we're using Colab, our Spark UI resides on Colab's local host. In order to make this available to us we the need the help of `ngrok`. This platform creates a secure tunnel to locally hosted applications using a reverse proxy. You don't need to complete this step but it's free and quite interesting."
      ],
      "metadata": {
        "id": "7k7KheNuy5Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print('Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken')\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4040\n",
        "public_url = ngrok.connect(ui_port).public_url\n",
        "print(f'ngrok tunnel {public_url}')"
      ],
      "metadata": {
        "id": "AqRb8ZO6uBjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "-bsEne5E2h1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll download a Plotly dataset to play around with."
      ],
      "metadata": {
        "id": "AarPiZzr2_BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the oil and gas dataset\n",
        "!wget https://raw.githubusercontent.com/plotly/datasets/master/oil-and-gas.parquet --quiet"
      ],
      "metadata": {
        "id": "CNNlJKg0v_DL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in the dataset\n",
        "df = spark.read.parquet('oil-and-gas.parquet')"
      ],
      "metadata": {
        "id": "BT6BpGzPr1Ko"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about the dataset\n",
        "print(f'Number of rows: {df.count()}')\n",
        "print(f'Number of columns: {len(df.columns)}')\n",
        "print('')\n",
        "print('Column name, type:')\n",
        "for i in range(0, len(df.columns)):\n",
        "  print(' ', f'{df.columns[i]}', ', ', f'{df.dtypes[i][1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoO54apvtqu3",
        "outputId": "c30d59fe-fef1-46c7-f313-1932903d26b7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 260374\n",
            "Number of columns: 11\n",
            "\n",
            "Column names, type:\n",
            "  Reporting Year ,  bigint\n",
            "  Gas Produced, MCF ,  double\n",
            "  Water Produced, bbl ,  double\n",
            "  Oil Produced, bbl ,  double\n",
            "  Surface Longitude ,  double\n",
            "  Surface Latitude ,  double\n",
            "  Well Name ,  string\n",
            "  Well Type ,  string\n",
            "  Date Well Completed ,  timestamp_ntz\n",
            "  Well Status ,  string\n",
            "  __index_level_0__ ,  bigint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about the columns\n",
        "print('Column name, no. unique levels:')\n",
        "for i in df.columns:\n",
        "  print(' ', i, ', ', df.select(i).distinct().count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxfGVH5E3WYz",
        "outputId": "7ca0df63-0866-4444-e9d8-d8d89d0b5313"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column name, no. unique levels\n",
            "  Reporting Year ,  31\n",
            "  Gas Produced, MCF ,  16828\n",
            "  Water Produced, bbl ,  1730\n",
            "  Oil Produced, bbl ,  910\n",
            "  Surface Longitude ,  13469\n",
            "  Surface Latitude ,  11930\n",
            "  Well Name ,  14228\n",
            "  Well Type ,  17\n",
            "  Date Well Completed ,  8125\n",
            "  Well Status ,  15\n",
            "  __index_level_0__ ,  260374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about missing data\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "total_rows = df.count()\n",
        "missing_counts = df.select([col(column).isNull().cast('int').alias(column) for column in df.columns])\n",
        "\n",
        "print('Column name, no. missing entries:')\n",
        "for i in df.columns:\n",
        "  missing_sum = missing_counts.agg(sum(i)).collect()[0][0]\n",
        "  print(' ', i, ', ', missing_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBdM0sgK4qqb",
        "outputId": "ae5589ca-0679-4c0b-e42e-5e7e3bb702c9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column name, no. missing entries:\n",
            "  Reporting Year ,  0\n",
            "  Gas Produced, MCF ,  0\n",
            "  Water Produced, bbl ,  0\n",
            "  Oil Produced, bbl ,  0\n",
            "  Surface Longitude ,  0\n",
            "  Surface Latitude ,  0\n",
            "  Well Name ,  0\n",
            "  Well Type ,  0\n",
            "  Date Well Completed ,  0\n",
            "  Well Status ,  0\n",
            "  __index_level_0__ ,  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about columns levels\n",
        "df.groupBy('Well Type').count().orderBy(col('count').desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlQR1esw8DmJ",
        "outputId": "d5e61ed4-298e-4b2d-946c-4759f7f8aa10"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|           Well Type| count|\n",
            "+--------------------+------+\n",
            "|     Gas Development|186942|\n",
            "|     Oil Development| 52142|\n",
            "|         Gas Wildcat|  7376|\n",
            "|       Gas Extension|  5737|\n",
            "|       Oil Injection|  4707|\n",
            "|         Oil Wildcat|  1288|\n",
            "|         Dry Wildcat|   879|\n",
            "|            Dry Hole|   511|\n",
            "|  Monitoring Storage|   308|\n",
            "|             Storage|   153|\n",
            "|          Not Listed|    90|\n",
            "|       Oil Extension|    82|\n",
            "|            Disposal|    58|\n",
            "|       Stratigraphic|    56|\n",
            "|          Geothermal|    24|\n",
            "|Monitoring Miscel...|    11|\n",
            "|               Brine|    10|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about columns levels\n",
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "print(f\"Earliest Well completion in dataset: {df.agg(min('Date Well Completed')).collect()[0][0]}\")\n",
        "print(f\"Latest Well completion in dataset: {df.agg(max('Date Well Completed')).collect()[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7FXSAS681WI",
        "outputId": "7e9981ad-649e-4983-b477-71621b200152"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earliest Well completion in dataset: 1881-08-01 00:00:00\n",
            "Latest Well completion in dataset: 2016-06-07 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y5zEmDir8_z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}